# BERT-large on Habana Gaudi

The BERT-large benchmark is sourced from the [HabanaAI Model References Github Repo](https://github.com/HabanaAI/Model-References/blob/master/README.md)

We utilized Gaudi2 systems. We had limited access to Intel's Habana Gaudi2 systems and conducted the experiments on their platform.

HabanaAI published optimized ResNet50 and BERT_large models for MLPERF3.1, which we have used in our paper; however, those models were not available on MLPERF4.0. Currently, they only have this [BERT-large](https://github.com/HabanaAI/Model-References/blob/master/PyTorch/nlp/bert/README.md) model available.
